{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc5a20-2109-4e5e-bf65-a48c8e36caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different llms other than GPT include, Bert, Llamam. Megatron-LM, Bart, Albert, T5, Roberta etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c94cd3a-ca1a-4b8f-9933-9a16d3e52aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT llms can be accessed via the openai Apis. Other models can be accessed via the Hugging Face APi\n",
    "# Hugging Face is an open source organization started in 2016 that focuses on NLP and Deep learning. It has state of the art tools and libraries\n",
    "# that make it easier for developers and researchers to work with NLP models including pre-trained models and for training your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf2087-fdd8-4089-9d87-ca0139a07706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most well known package on Hugging face is the the transformer library for working with llm in python- access to bert, gpt2, roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9982bf-cd75-4b07-a67a-525b0e1e10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple api for loading and using a pretrained model, tokenize text, make predictions, generate text -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b02a3e-0510-4770-9b7c-d1653a01a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis, named entity recognition, text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24384d9-919d-4009-921c-76b8efe536e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers library integrates with pytorch and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228eaa2-d023-46ee-b3c4-ffa93fc4ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers pipeline Input -> Tokenizer -> Model -> Post processing -> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8255f5a5-7c1a-4a31-8245-2b6972c91315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87831347-d40c-48aa-994e-041e4ba89b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#sentiment_classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b7257-b08b-461b-b537-930329b11b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment_classifier(\"I am so excited to working on llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe43ba-7518-4f31-ae21-d4896f1a0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My kernel dies if i use the default model associated with the sentiment-analysis pipeline. So I am going to try a smaller model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da6b872-b9ed-47f4-94e2-f25e026b2979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_classifier_small = pipeline(\"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517893c-0ed2-4d4d-b855-ab5ee21ba6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_classifier_small(\"I am so excited to working on llms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66a36815-e3f7-4cf2-bb3d-cd6c708fa3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7870fd23776a467abaa77a31e9d34cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6165d867763e49a1b12ce493133a0cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77cc90e33e34f30be5fbd16a3ff7460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f99ad53479f43e68e70c7a46e0d0342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b224f7b015464cc094ed4f7766c6b4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce942815dad4e37b77884279c9ce0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e8c48d6-8f88-4652-b984-87fe20f1c9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.99427235,\n",
       "  'index': 4,\n",
       "  'word': 'Anna',\n",
       "  'start': 12,\n",
       "  'end': 16},\n",
       " {'entity': 'B-LOC',\n",
       "  'score': 0.986809,\n",
       "  'index': 9,\n",
       "  'word': 'Boston',\n",
       "  'start': 34,\n",
       "  'end': 40}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"Her name is Anna and she works in Boston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b553dba-8b24-4fa4-bbd3-81a964697552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Huggin face you can filter models based on tasks. Lets try a zero shot model. Trying on calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c895cb4-6955-4edd-aab2-87417f087479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eef76b7f2864ef5b947d905e0886e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10975df5461345e1bed768617a2939cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53f05fef6de4cd5bd1c2a5569c3a13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7eea60bb945415a92977945a98fda5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e41f9ea9f7a40a9b4988a32e9b1c407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09ab690809d4a35b7b4b6411151bdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "zeroshot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04bc2190-6c82-4f19-a8bf-39f049873f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \" one day i will see the world\"\n",
    "candidate_labels = ['trave', 'cooking', 'dancing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53fc64-18b2-4582-bfc5-f7f0ea7f194e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_classifier(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03fd422-2c33-4acb-8ea8-4ebca2b61736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel keeps failing a lot of these models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26fb628-66d8-4dab-bc43-39ac15625ead",
   "metadata": {},
   "source": [
    "# Pre-Trained Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da0b73f4-9363-456e-9628-6bfa0d01f3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec70e33-fc83-4ac8-b6f6-91b26bab2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"bert-base-uncased\" # specifying the model ensures that tokenization happens specific to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23fe9f11-f57b-4c8d-9010-56c8c7ccefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f753b3d-308c-49a0-ba82-fc0bd0c184e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =\" I am so excited to be learning about llms. Are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e14ffbe3-a775-401f-b5aa-bbe47b92cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e7320b7-7d5b-4ff3-aa4a-bb2c80afec06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1045, 2572, 2061, 7568, 2000, 2022, 4083, 2055, 2222, 5244, 1012, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print( input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e88c50-cf21-4477-99d0-59974c5e1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids , token_type_ids are used in some models to distinguish one sentence from another 0 would belong to first senntence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921fd91f-cb0e-4e11-9621-3c155bc5eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1 to classification or second sentend c,  attention_mask indicates which word to pay attention to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c2d0d-b959-4c20-869d-442f08cec4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer breaks out the steps into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "74bbe3ee-f1e6-436f-ace6-b90359c7da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3dffc1f3-1c2d-4ffc-9a76-6a57f3837eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'so', 'excited', 'to', 'be', 'learning', 'about', 'll', '##ms', '.', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c94840b2-56ce-4798-861f-00702fb21f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens are then converted into numerical value from a predefined vocabulary  based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f20c5959-02c9-41e6-9055-a0043c50d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7d343e8-3854-4333-bb3a-3b94a2b285e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1045, 2572, 2061, 7568, 2000, 2022, 4083, 2055, 2222, 5244, 1012, 2024, 2017, 1029]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd7f1e-bd12-497b-aa10-97d2408d0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the token ids back into tokens to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd752ff8-89dd-4ad8-9d93-26d00ac7bd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_token_ids = tokenizer.convert_ids_to_tokens( token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2fcdab9f-72a9-470e-b18f-1d08643029b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'so', 'excited', 'to', 'be', 'learning', 'about', 'll', '##ms', '.', 'are', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "print(decode_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c9523b75-8080-47eb-861d-2c397500bacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "20578ba3-9ea3-477a-b337-67550938faf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b92182-8535-4a1e-a08c-25b064d1fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are specials tokens added by out tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f80d786-8b90-4a78-a452-c7904cbed4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XLNet is another llm that differs in its tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ff287b1-f57c-48c0-92db-0851eee1b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = \"xlnet-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6fa450d2-59f3-427e-ab4f-67060ff4ccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b766f35081648098a22ae398c4a4299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eec2daabdb447fa825018198d863c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2c95d8c75a4eb0a7c157123cd07c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d549e72f-773b-4f83-9130-1709dd79ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer2(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c3d6df12-9288-435c-bbe9-1880189d1e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [35, 569, 102, 5564, 22, 39, 1899, 75, 17, 215, 98, 23, 9, 3129, 44, 82, 4, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3757b399-f180-434d-abf2-9b2f2add933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer2.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce705299-91f0-4dbf-ae6e-ee23132bc92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 569, 102, 5564, 22, 39, 1899, 75, 17, 215, 98, 23, 9, 3129, 44, 82]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21668d27-9a16-4f8f-b21f-ba333b7c9dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ids differs from the bert model.  bert model began the tokens with the cls special token but xlnet does not beging with this token at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a247e25-e52d-49c2-905e-c353b596b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead we see 2 extra numbers at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02418aa4-c463-4c41-bcb1-d53696cb7a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sep>'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7af0dff2-4a8b-4fbe-ac2a-44a930e25b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<cls>'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450bab0-bb3d-4849-b495-f40e4e579b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WhaT ARE the special tokens and why are they added to our input sequence. They are specific markers that help the model perform various task\n",
    "# Guides the model behaviour\n",
    "# CLS is classification tag - generally placed at beginning of input\n",
    "# SEP is the seperator - used to seperate segments\n",
    "# used in text classification and sentence pair classification. Determine relationship btween sentences. passeges classification\n",
    "# MASK token is used Masked language modelling or text generation with a blank to fill in. Ex: Models task to fill in the missing word here:\n",
    "# Fine tuning is [MASK] for all. Can be used for text completition\n",
    "# You can also designate special token's to guide model behavior. Example [SOURCE] and [TARGET] for translation\n",
    "# [PAD] to bring all sentences are same length\n",
    "# [TRUNCATE] to bring down the size of the inputs\n",
    "# These tokens help aling the inputs with the model's architecture\n",
    "# This is why it is important to specify a model when working with a tokenizer so that it processes the text in that format for that model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce75b37a-7c30-40b4-8b5e-0bf92e206544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face / Pytorch - Tensorflow - fine tuning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa73f2f-e4a3-40ce-bab5-b2235aafdef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5afcbb-2d84-4f23-b126-e83809b0806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7e3db74-2d1c-4e8b-bfce-3073bc889a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am so excited to be learning about llms. Are you?\n",
      "{'input_ids': [101, 1045, 2572, 2061, 7568, 2000, 2022, 4083, 2055, 2222, 5244, 1012, 2024, 2017, 1029, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2475774c-9bf9-4e6c-9dac-55f482f403d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get our output in the correct format for pytorch, we need to convert our input embeddings into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8964ac4a-3b02-40fe-899b-4ffaf75f8d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\") # pretrained model that can classify text as + or -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a624b57-523c-4e56-8213-c4e4bff1e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_pt = tokenizer(sentence, return_tensors=\"pt\") # return input ids as a pytoch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86509425-2071-47bb-aa99-04d8eb12f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 1045, 2572, 2061, 7568, 2000, 2022, 4083, 2055, 2222, 5244, 1012,\n",
      "         2024, 2017, 1029,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f7c04-f6a4-4026-8eab-96d35f49a8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is the same but it is now formatted within a pytorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0322eed1-2a18-45e4-9bdf-633c1772fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24979327-43b4-49f3-9184-836866a05f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # disables gradient calculation which is mainly used for training and also consumes lot of memory\n",
    "    logits = model(**input_ids_pt).logits # call the model with tensors to make the prediction\n",
    "\n",
    "predicted_class_id = logits.argmax().item() # identifies the class with the highest predicted score\n",
    "mode.config.id2label(predicted_class_id) # converts the class id into a human redable label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb736c-fc3f-458d-83fb-e14fe734dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not able to run this model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37052b6-6d6d-4d0f-ac47-06941b5f3a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01fb8447-7752-4556-8428-2212efae87c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"my_saved_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3e6a73e-f4e8-4f4c-841f-2d0c8f5d06a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_saved_models/tokenizer_config.json',\n",
       " 'my_saved_models/special_tokens_map.json',\n",
       " 'my_saved_models/vocab.txt',\n",
       " 'my_saved_models/added_tokens.json',\n",
       " 'my_saved_models/tokenizer.json')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01a26bde-ccd8-466c-bd16-7de470e0b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2229ce6-d720-440d-b114-c51fdec56668",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer = AutoTokenizer.from_pretrained(model_directory)\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a1d91dd-b658-40ab-87b9-26e20848d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retieving from the saved model dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7029ec-15d5-4fc2-bb22-06a6c7152c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
